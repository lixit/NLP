{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing text into sentances using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n",
      "mention her under any other name. In his eyes she eclipses and\n",
      "predominates the whole of her sex. It was not that he felt any emotion\n",
      "akin to love for Irene Adler. All emotions, and that one particularly,\n",
      "were abhorrent to his cold, precise but admirably balanced mind. He\n",
      "was, I take it, the most perfect reasoning and observing machine that\n",
      "the world has seen, but as a lover he would have placed himself in a\n",
      "false position. He never spoke of the softer passions, save with a gibe\n",
      "and a sneer. They were admirable things for the observer—excellent for\n",
      "drawing the veil from men’s motives and actions. But for the trained\n",
      "reasoner to admit such intrusions into his own delicate and finely\n",
      "adjusted temperament was to introduce a distracting factor which might\n",
      "throw a doubt upon all his mental results. Grit in a sensitive\n",
      "instrument, or a crack in one of his own high-power lenses, would not\n",
      "be more disturbing than a strong emotion in a nature such as his. And\n",
      "yet there was but one woman to him, and that woman was the late Irene\n",
      "Adler, of dubious and questionable memory.\n",
      "\n",
      "Sentence 1 : To Sherlock Holmes she is always _the_ woman.\n",
      "Sentence 2 : I have seldom heard him mention her under any other name.\n",
      "Sentence 3 : In his eyes she eclipses and predominates the whole of her sex.\n",
      "Sentence 4 : It was not that he felt any emotion akin to love for Irene Adler.\n",
      "Sentence 5 : All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.\n",
      "Sentence 6 : He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.\n",
      "Sentence 7 : He never spoke of the softer passions, save with a gibe and a sneer.\n",
      "Sentence 8 : They were admirable things for the observer—excellent for drawing the veil from men’s motives and actions.\n",
      "Sentence 9 : But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.\n",
      "Sentence 10 : Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his.\n",
      "Sentence 11 : And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "filename = \"sherlock_holmes_1.txt\"\n",
    "file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "print(text)\n",
    "print()\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "sentences = tokenizer.tokenize(text)\n",
    "\n",
    "for (i, sentence) in enumerate(sentences):\n",
    "    print(\"Sentence\", i + 1, \":\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing text into sentances using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n",
      "mention her under any other name. In his eyes she eclipses and\n",
      "predominates the whole of her sex. It was not that he felt any emotion\n",
      "akin to love for Irene Adler. All emotions, and that one particularly,\n",
      "were abhorrent to his cold, precise but admirably balanced mind. He\n",
      "was, I take it, the most perfect reasoning and observing machine that\n",
      "the world has seen, but as a lover he would have placed himself in a\n",
      "false position. He never spoke of the softer passions, save with a gibe\n",
      "and a sneer. They were admirable things for the observer—excellent for\n",
      "drawing the veil from men’s motives and actions. But for the trained\n",
      "reasoner to admit such intrusions into his own delicate and finely\n",
      "adjusted temperament was to introduce a distracting factor which might\n",
      "throw a doubt upon all his mental results. Grit in a sensitive\n",
      "instrument, or a crack in one of his own high-power lenses, would not\n",
      "be more disturbing than a strong emotion in a nature such as his. And\n",
      "yet there was but one woman to him, and that woman was the late Irene\n",
      "Adler, of dubious and questionable memory.\n",
      "['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him mention her under any other name.', 'In his eyes she eclipses and predominates the whole of her sex.', 'It was not that he felt any emotion akin to love for Irene Adler.', 'All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.', 'He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.', 'He never spoke of the softer passions, save with a gibe and a sneer.', 'They were admirable things for the observer—excellent for drawing the veil from men’s motives and actions.', 'But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.', 'Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his.', 'And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "filename = \"sherlock_holmes_1.txt\"\n",
    "file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "print(text)\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "sentences = [sentence.text for sentence in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing sentences into words – tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_the_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer—excellent', 'for', 'drawing', 'the', 'veil', 'from', 'men', '’', 's', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high-power', 'lenses', ',', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']\n",
      "['Central', 'Park', 'Tower', 'is', 'reaaally', 'hiiigh']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "filename = \"sherlock_holmes_1.txt\"\n",
    "file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "words = nltk.tokenize.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "\n",
    "tweet = \"@EmpireStateBldg Central Park Tower is reaaaally hiiiigh\"\n",
    "\n",
    "words = nltk.tokenize.casual.casual_tokenize(tweet,\n",
    "                                                preserve_case=True,\n",
    "                                                reduce_len=True,\n",
    "                                                strip_handles=True)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_', 'the', '_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer', '—', 'excellent', 'for', 'drawing', 'the', 'veil', 'from', 'men', '’s', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high', '-', 'power', 'lenses', ',', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']\n",
      "['ADP', 'PROPN', 'PROPN', 'PRON', 'AUX', 'ADV', 'PUNCT', 'DET', 'PROPN', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'ADV', 'VERB', 'PRON', 'VERB', 'PRON', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'ADP', 'PRON', 'NOUN', 'PRON', 'VERB', 'CCONJ', 'VERB', 'DET', 'NOUN', 'ADP', 'PRON', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'PART', 'SCONJ', 'PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'PART', 'VERB', 'ADP', 'PROPN', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'DET', 'NUM', 'ADV', 'PUNCT', 'AUX', 'ADJ', 'ADP', 'PRON', 'ADJ', 'PUNCT', 'ADJ', 'CCONJ', 'ADV', 'ADJ', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'PUNCT', 'PRON', 'VERB', 'PRON', 'PUNCT', 'DET', 'ADV', 'ADJ', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'PRON', 'DET', 'NOUN', 'AUX', 'VERB', 'PUNCT', 'CCONJ', 'ADP', 'DET', 'NOUN', 'PRON', 'AUX', 'AUX', 'VERB', 'PRON', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'PRON', 'ADV', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'VERB', 'ADP', 'DET', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT', 'ADJ', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'PART', 'NOUN', 'CCONJ', 'NOUN', 'PUNCT', 'CCONJ', 'SCONJ', 'DET', 'VERB', 'NOUN', 'PART', 'VERB', 'ADJ', 'NOUN', 'ADP', 'PRON', 'ADJ', 'ADJ', 'CCONJ', 'ADV', 'VERB', 'NOUN', 'AUX', 'PART', 'VERB', 'DET', 'VERB', 'NOUN', 'PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'SCONJ', 'DET', 'PRON', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'CCONJ', 'DET', 'NOUN', 'ADP', 'NUM', 'ADP', 'PRON', 'ADJ', 'ADJ', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'AUX', 'PART', 'AUX', 'ADV', 'ADJ', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADJ', 'ADP', 'PRON', 'PUNCT', 'CCONJ', 'ADV', 'PRON', 'VERB', 'CCONJ', 'NUM', 'NOUN', 'ADP', 'PRON', 'PUNCT', 'CCONJ', 'DET', 'NOUN', 'AUX', 'DET', 'ADJ', 'PROPN', 'PROPN', 'PUNCT', 'ADP', 'ADJ', 'CCONJ', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "filename = \"sherlock_holmes_1.txt\"\n",
    "file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "words = [token.text for token in doc]\n",
    "pos = [token.pos_ for token in doc]\n",
    "word_pos_tuples = [(token.text, token.pos_) for token in doc]\n",
    "print(words)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leaf', 'leav', 'book', 'write', 'complet', 'stem', 'sky']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = ['leaf', 'leaves', 'booking', 'writing', 'completed', 'stemming', 'skies']\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['duck', 'goose', 'cat', 'book']\n",
      "love\n",
      "bad\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['duck', 'geese', 'cats', 'books']\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(lemmatized_words)\n",
    "\n",
    "print(lemmatizer.lemmatize('loved', 'v')) # part of speech tag: v for verb\n",
    "print(lemmatizer.lemmatize('worse', 'a')) # part of speech tag: a for adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('To', 'To'), ('Sherlock', 'Sherlock'), ('Holmes', 'Holmes'), ('she', 'she'), ('is', 'v'), ('always', 'always'), ('_the_', 'a'), ('woman', 'n'), ('.', '.'), ('I', 'I'), ('have', 'v'), ('seldom', 'v'), ('heard', 'heard'), ('him', 'him'), ('mention', 'mention'), ('her', 'her'), ('under', 'under'), ('any', 'any'), ('other', 'a'), ('name', 'n'), ('.', '.'), ('In', 'In'), ('his', 'his'), ('eye', 'n'), ('she', 'she'), ('eclipse', 'v'), ('and', 'and'), ('predominates', 'v'), ('the', 'the'), ('whole', 'n'), ('of', 'of'), ('her', 'her'), ('sex', 'n'), ('.', '.'), ('It', 'It'), ('wa', 'v'), ('not', 'not'), ('that', 'that'), ('he', 'he'), ('felt', 'v'), ('any', 'any'), ('emotion', 'n'), ('akin', 'n'), ('to', 'to'), ('love', 'love'), ('for', 'for'), ('Irene', 'Irene'), ('Adler', 'Adler'), ('.', '.'), ('All', 'All'), ('emotion', 'n'), (',', ','), ('and', 'and'), ('that', 'that'), ('one', 'one'), ('particularly', 'particularly'), (',', ','), ('were', 'v'), ('abhorrent', 'a'), ('to', 'to'), ('his', 'his'), ('cold', 'n'), (',', ','), ('precise', 'n'), ('but', 'but'), ('admirably', 'admirably'), ('balanced', 'v'), ('mind', 'n'), ('.', '.'), ('He', 'He'), ('wa', 'v'), (',', ','), ('I', 'I'), ('take', 'v'), ('it', 'it'), (',', ','), ('the', 'the'), ('most', 'most'), ('perfect', 'a'), ('reasoning', 'n'), ('and', 'and'), ('observing', 'v'), ('machine', 'n'), ('that', 'that'), ('the', 'the'), ('world', 'n'), ('ha', 'v'), ('seen', 'v'), (',', ','), ('but', 'but'), ('as', 'as'), ('a', 'a'), ('lover', 'n'), ('he', 'he'), ('would', 'would'), ('have', 'have'), ('placed', 'v'), ('himself', 'himself'), ('in', 'in'), ('a', 'a'), ('false', 'a'), ('position', 'n'), ('.', '.'), ('He', 'He'), ('never', 'never'), ('spoke', 'v'), ('of', 'of'), ('the', 'the'), ('softer', 'a'), ('passion', 'n'), (',', ','), ('save', 'v'), ('with', 'with'), ('a', 'a'), ('gibe', 'n'), ('and', 'and'), ('a', 'a'), ('sneer', 'n'), ('.', '.'), ('They', 'They'), ('were', 'v'), ('admirable', 'a'), ('thing', 'n'), ('for', 'for'), ('the', 'the'), ('observer—excellent', 'n'), ('for', 'for'), ('drawing', 'v'), ('the', 'the'), ('veil', 'n'), ('from', 'from'), ('men', 'n'), ('’', 'v'), ('s', 'a'), ('motif', 'n'), ('and', 'and'), ('action', 'n'), ('.', '.'), ('But', 'But'), ('for', 'for'), ('the', 'the'), ('trained', 'a'), ('reasoner', 'n'), ('to', 'to'), ('admit', 'admit'), ('such', 'a'), ('intrusion', 'n'), ('into', 'into'), ('his', 'his'), ('own', 'a'), ('delicate', 'n'), ('and', 'and'), ('finely', 'finely'), ('adjusted', 'v'), ('temperament', 'n'), ('wa', 'v'), ('to', 'to'), ('introduce', 'introduce'), ('a', 'a'), ('distracting', 'n'), ('factor', 'n'), ('which', 'which'), ('might', 'might'), ('throw', 'throw'), ('a', 'a'), ('doubt', 'n'), ('upon', 'upon'), ('all', 'all'), ('his', 'his'), ('mental', 'a'), ('result', 'n'), ('.', '.'), ('Grit', 'Grit'), ('in', 'in'), ('a', 'a'), ('sensitive', 'a'), ('instrument', 'n'), (',', ','), ('or', 'or'), ('a', 'a'), ('crack', 'n'), ('in', 'in'), ('one', 'one'), ('of', 'of'), ('his', 'his'), ('own', 'a'), ('high-power', 'n'), ('lens', 'n'), (',', ','), ('would', 'would'), ('not', 'not'), ('be', 'be'), ('more', 'more'), ('disturbing', 'a'), ('than', 'than'), ('a', 'a'), ('strong', 'a'), ('emotion', 'n'), ('in', 'in'), ('a', 'a'), ('nature', 'n'), ('such', 'a'), ('as', 'as'), ('his', 'his'), ('.', '.'), ('And', 'And'), ('yet', 'yet'), ('there', 'there'), ('wa', 'v'), ('but', 'but'), ('one', 'one'), ('woman', 'n'), ('to', 'to'), ('him', 'him'), (',', ','), ('and', 'and'), ('that', 'that'), ('woman', 'n'), ('wa', 'v'), ('the', 'the'), ('late', 'a'), ('Irene', 'Irene'), ('Adler', 'Adler'), (',', ','), ('of', 'of'), ('dubious', 'a'), ('and', 'and'), ('questionable', 'a'), ('memory', 'n'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/a/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tag_nltk(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "def read_text_file(filename):\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    return text\n",
    "\n",
    "pos_mapping = {'JJ':'a', 'JJR':'a', 'JJS':'a', 'NN':'n',\n",
    "    'NNS':'n', 'VBD':'v', 'VBG':'v', 'VBN':'v', 'VBP':'v', 'VBZ':'v'}\n",
    "accepted_pos = {'a', 'v', 'n'}\n",
    "\n",
    "def lemmatize_long_text(text):\n",
    "    words = pos_tag_nltk(text)\n",
    "    words = [(word_tuple[0], pos_mapping[word_tuple[1]] if word_tuple[1] in pos_mapping.keys()\n",
    "              else word_tuple[0], word_tuple[1]) for word_tuple in words]\n",
    "    words = [(lemmatizer.lemmatize(word_tuple[0]) if word_tuple[1] in accepted_pos else word_tuple[0], word_tuple[1]) for word_tuple in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "text = read_text_file(\"sherlock_holmes_1.txt\")\n",
    "lem_words = lemmatize_long_text(text)\n",
    "print(lem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sherlock', 'Holmes', '_the_', 'woman', '.', 'seldom', 'heard', 'mention', 'name', '.', 'eyes', 'eclipses', 'predominates', 'whole', 'sex', '.', 'felt', 'emotion', 'akin', 'love', 'Irene', 'Adler', '.', 'emotions', ',', ',', 'abhorrent', 'cold', ',', 'precise', 'admirably', 'balanced', 'mind', '.', ',', 'take', ',', 'perfect', 'reasoning', 'observing', 'machine', 'world', ',', 'lover', 'placed', 'false', 'position', '.', 'spoke', 'softer', 'passions', ',', 'save', 'gibe', 'sneer', '.', 'admirable', 'observer—excellent', 'drawing', 'veil', 'men', '’', 'motives', 'actions', '.', 'trained', 'reasoner', 'admit', 'intrusions', 'own', 'delicate', 'finely', 'adjusted', 'temperament', 'introduce', 'distracting', 'factor', 'throw', 'doubt', 'mental', 'results', '.', 'Grit', 'sensitive', 'instrument', ',', 'crack', 'own', 'high-power', 'lenses', ',', 'disturbing', 'strong', 'emotion', 'nature', '.', 'woman', ',', 'woman', 'late', 'Irene', 'Adler', ',', 'dubious', 'questionable', 'memory', '.']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "csv_file = \"stopwords.csv\"\n",
    "\n",
    "with open(csv_file, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    stopwords = [row[0] for row in reader]\n",
    "    \n",
    "# stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "# print(stopwords)\n",
    "\n",
    "filename = \"sherlock_holmes_1.txt\"\n",
    "file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "words = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "words = [word for word in words if word.lower() not in stopwords]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords - FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 11), (',', 11), ('a', 10), ('and', 9), ('the', 8), ('to', 6), ('his', 6), ('in', 5), ('was', 5), ('of', 4)]\n",
      "[('to', 6), ('sherlock', 1), ('holmes', 1), ('she', 2), ('is', 1), ('always', 1), ('_the_', 1), ('woman', 3), ('.', 11), ('i', 2), ('have', 2), ('seldom', 1), ('heard', 1), ('him', 2), ('mention', 1), ('her', 2), ('under', 1), ('any', 2), ('other', 1), ('name', 1), ('in', 5), ('his', 6), ('eyes', 1), ('eclipses', 1), ('and', 9), ('predominates', 1), ('the', 8), ('whole', 1), ('of', 4), ('sex', 1), ('it', 2), ('was', 5), ('not', 2), ('that', 4), ('he', 4), ('felt', 1), ('emotion', 2), ('akin', 1), ('love', 1), ('for', 4), ('irene', 2), ('adler', 2), ('all', 2), ('emotions', 1), (',', 11), ('one', 3), ('particularly', 1), ('were', 2), ('abhorrent', 1), ('cold', 1), ('precise', 1), ('but', 4), ('admirably', 1), ('balanced', 1), ('mind', 1), ('take', 1), ('most', 1), ('perfect', 1), ('reasoning', 1), ('observing', 1), ('machine', 1), ('world', 1), ('has', 1), ('seen', 1), ('as', 2), ('a', 10), ('lover', 1), ('would', 2), ('placed', 1), ('himself', 1), ('false', 1), ('position', 1), ('never', 1), ('spoke', 1), ('softer', 1), ('passions', 1), ('save', 1), ('with', 1), ('gibe', 1), ('sneer', 1), ('they', 1), ('admirable', 1), ('things', 1), ('observer—excellent', 1), ('drawing', 1), ('veil', 1), ('from', 1), ('men', 1), ('’', 1), ('s', 1), ('motives', 1), ('actions', 1), ('trained', 1), ('reasoner', 1), ('admit', 1), ('such', 2), ('intrusions', 1), ('into', 1), ('own', 2), ('delicate', 1), ('finely', 1), ('adjusted', 1), ('temperament', 1), ('introduce', 1), ('distracting', 1), ('factor', 1), ('which', 1), ('might', 1), ('throw', 1), ('doubt', 1), ('upon', 1), ('mental', 1), ('results', 1), ('grit', 1), ('sensitive', 1), ('instrument', 1), ('or', 1), ('crack', 1), ('high-power', 1), ('lenses', 1), ('be', 1), ('more', 1), ('disturbing', 1), ('than', 1), ('strong', 1), ('nature', 1), ('yet', 1), ('there', 1), ('late', 1), ('dubious', 1), ('questionable', 1), ('memory', 1)]\n",
      "[('sherlock', 1), ('holmes', 1), ('is', 1), ('always', 1), ('_the_', 1), ('seldom', 1), ('heard', 1), ('mention', 1), ('under', 1), ('other', 1), ('name', 1), ('eyes', 1), ('eclipses', 1), ('predominates', 1), ('whole', 1), ('sex', 1), ('felt', 1), ('akin', 1), ('love', 1), ('emotions', 1), ('particularly', 1), ('abhorrent', 1), ('cold', 1), ('precise', 1), ('admirably', 1), ('balanced', 1), ('mind', 1), ('take', 1), ('most', 1), ('perfect', 1), ('reasoning', 1), ('observing', 1), ('machine', 1), ('world', 1), ('has', 1), ('seen', 1), ('lover', 1), ('placed', 1), ('himself', 1), ('false', 1), ('position', 1), ('never', 1), ('spoke', 1), ('softer', 1), ('passions', 1), ('save', 1), ('with', 1), ('gibe', 1), ('sneer', 1), ('they', 1), ('admirable', 1), ('things', 1), ('observer—excellent', 1), ('drawing', 1), ('veil', 1), ('from', 1), ('men', 1), ('’', 1), ('s', 1), ('motives', 1), ('actions', 1), ('trained', 1), ('reasoner', 1), ('admit', 1), ('intrusions', 1), ('into', 1), ('delicate', 1), ('finely', 1), ('adjusted', 1), ('temperament', 1), ('introduce', 1), ('distracting', 1), ('factor', 1), ('which', 1), ('might', 1), ('throw', 1), ('doubt', 1), ('upon', 1), ('mental', 1), ('results', 1), ('grit', 1), ('sensitive', 1), ('instrument', 1), ('or', 1), ('crack', 1), ('high-power', 1), ('lenses', 1), ('be', 1), ('more', 1), ('disturbing', 1), ('than', 1), ('strong', 1), ('nature', 1), ('yet', 1), ('there', 1), ('late', 1), ('dubious', 1), ('questionable', 1), ('memory', 1), ('she', 2), ('i', 2), ('have', 2), ('him', 2), ('her', 2), ('any', 2), ('it', 2), ('not', 2), ('emotion', 2), ('irene', 2), ('adler', 2), ('all', 2), ('were', 2), ('as', 2), ('would', 2), ('such', 2), ('own', 2), ('woman', 3), ('one', 3), ('of', 4), ('that', 4), ('he', 4), ('for', 4), ('but', 4), ('in', 5), ('was', 5), ('to', 6), ('his', 6), ('the', 8), ('and', 9), ('a', 10), ('.', 11), (',', 11)]\n",
      "['.', ',']\n",
      "['.', ',']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "filename = \"sherlock_holmes_1.txt\"\n",
    "file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "freq_dist = FreqDist(word.lower() for word in words)\n",
    "\n",
    "print(freq_dist.most_common(10))\n",
    "\n",
    "words_with_frequencies = [(word, freq_dist[word]) for word in freq_dist.keys()]\n",
    "\n",
    "print(words_with_frequencies)\n",
    "\n",
    "sorted_words = sorted(words_with_frequencies, key=lambda x: x[1])\n",
    "\n",
    "print(sorted_words)\n",
    "\n",
    "# Option 1: use the n most frequent words as stopwords\n",
    "stopwords = [word[0] for word in sorted_words if word[1] > 10]\n",
    "\n",
    "print(stopwords)\n",
    "\n",
    "# Option 2: use the n% most frequent words as stopwords\n",
    "length_cutoff = int(0.02 * len(sorted_words))\n",
    "stopwords = [tuple[0] for tuple in sorted_words[-length_cutoff:]]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting nouns – plural and singular nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('To', 'TO'), ('Sherlock', 'NNP'), ('Holmes', 'NNP'), ('she', 'PRP'), ('is', 'VBZ'), ('always', 'RB'), ('_the_', 'JJ'), ('woman', 'NN'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('seldom', 'VBN'), ('heard', 'RB'), ('him', 'PRP'), ('mention', 'VB'), ('her', 'PRP'), ('under', 'IN'), ('any', 'DT'), ('other', 'JJ'), ('name', 'NN'), ('.', '.'), ('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('felt', 'VBD'), ('any', 'DT'), ('emotion', 'NN'), ('akin', 'NN'), ('to', 'TO'), ('love', 'VB'), ('for', 'IN'), ('Irene', 'NNP'), ('Adler', 'NNP'), ('.', '.'), ('All', 'DT'), ('emotions', 'NNS'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('one', 'CD'), ('particularly', 'RB'), (',', ','), ('were', 'VBD'), ('abhorrent', 'JJ'), ('to', 'TO'), ('his', 'PRP$'), ('cold', 'NN'), (',', ','), ('precise', 'NN'), ('but', 'CC'), ('admirably', 'RB'), ('balanced', 'VBD'), ('mind', 'NN'), ('.', '.'), ('He', 'PRP'), ('was', 'VBD'), (',', ','), ('I', 'PRP'), ('take', 'VBP'), ('it', 'PRP'), (',', ','), ('the', 'DT'), ('most', 'RBS'), ('perfect', 'JJ'), ('reasoning', 'NN'), ('and', 'CC'), ('observing', 'VBG'), ('machine', 'NN'), ('that', 'IN'), ('the', 'DT'), ('world', 'NN'), ('has', 'VBZ'), ('seen', 'VBN'), (',', ','), ('but', 'CC'), ('as', 'IN'), ('a', 'DT'), ('lover', 'NN'), ('he', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('placed', 'VBN'), ('himself', 'PRP'), ('in', 'IN'), ('a', 'DT'), ('false', 'JJ'), ('position', 'NN'), ('.', '.'), ('He', 'PRP'), ('never', 'RB'), ('spoke', 'VBD'), ('of', 'IN'), ('the', 'DT'), ('softer', 'JJR'), ('passions', 'NNS'), (',', ','), ('save', 'VBP'), ('with', 'IN'), ('a', 'DT'), ('gibe', 'NN'), ('and', 'CC'), ('a', 'DT'), ('sneer', 'NN'), ('.', '.'), ('They', 'PRP'), ('were', 'VBD'), ('admirable', 'JJ'), ('things', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('observer—excellent', 'NN'), ('for', 'IN'), ('drawing', 'VBG'), ('the', 'DT'), ('veil', 'NN'), ('from', 'IN'), ('men', 'NNS'), ('’', 'VBP'), ('s', 'JJ'), ('motives', 'NNS'), ('and', 'CC'), ('actions', 'NNS'), ('.', '.'), ('But', 'CC'), ('for', 'IN'), ('the', 'DT'), ('trained', 'JJ'), ('reasoner', 'NN'), ('to', 'TO'), ('admit', 'VB'), ('such', 'JJ'), ('intrusions', 'NNS'), ('into', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('delicate', 'NN'), ('and', 'CC'), ('finely', 'RB'), ('adjusted', 'VBD'), ('temperament', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('introduce', 'VB'), ('a', 'DT'), ('distracting', 'NN'), ('factor', 'NN'), ('which', 'WDT'), ('might', 'MD'), ('throw', 'VB'), ('a', 'DT'), ('doubt', 'NN'), ('upon', 'IN'), ('all', 'PDT'), ('his', 'PRP$'), ('mental', 'JJ'), ('results', 'NNS'), ('.', '.'), ('Grit', 'NNP'), ('in', 'IN'), ('a', 'DT'), ('sensitive', 'JJ'), ('instrument', 'NN'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('crack', 'NN'), ('in', 'IN'), ('one', 'CD'), ('of', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('high-power', 'NN'), ('lenses', 'NNS'), (',', ','), ('would', 'MD'), ('not', 'RB'), ('be', 'VB'), ('more', 'RBR'), ('disturbing', 'JJ'), ('than', 'IN'), ('a', 'DT'), ('strong', 'JJ'), ('emotion', 'NN'), ('in', 'IN'), ('a', 'DT'), ('nature', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('his', 'PRP$'), ('.', '.'), ('And', 'CC'), ('yet', 'RB'), ('there', 'EX'), ('was', 'VBD'), ('but', 'CC'), ('one', 'CD'), ('woman', 'NN'), ('to', 'TO'), ('him', 'PRP'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('woman', 'NN'), ('was', 'VBD'), ('the', 'DT'), ('late', 'JJ'), ('Irene', 'NNP'), ('Adler', 'NNP'), (',', ','), ('of', 'IN'), ('dubious', 'JJ'), ('and', 'CC'), ('questionable', 'JJ'), ('memory', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import inflect\n",
    "from nltk import pos_tag\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "file = open(\"sherlock_holmes_1.txt\", \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "words_with_pos = pos_tag(word_tokenize(text))\n",
    "\n",
    "print(words_with_pos)\n",
    "\n",
    "def get_nouns(words_with_pos):\n",
    "    nouns_set = ['NN', 'NNS']\n",
    "    nouns = [word for word in words_with_pos if word[1] in nouns_set]\n",
    "    return nouns\n",
    "\n",
    "def is_plural_nltk(noun_info):\n",
    "    pos = noun_info[1]\n",
    "    if pos == 'NNS':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_plural_wn(noun):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma = wnl.lemmatize(noun, 'n')\n",
    "    plural = True if noun is not lemma else False\n",
    "    \n",
    "    return plural\n",
    "\n",
    "def get_plural(singular_noun):\n",
    "    p = inflect.engine()\n",
    "    plural = p.plural(singular_noun)\n",
    "    return plural\n",
    "\n",
    "def get_singular(plural_noun):\n",
    "    p = inflect.engine()\n",
    "    singular = p.singular_noun(plural_noun)\n",
    "    return singular\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dependency parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "sentence = 'I have seldom heard him mention her under any other name.'\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, '\\t', token.dep_, \"\\t\", spacy.explain(token.dep_))\n",
    "    \n",
    "    \n",
    "# for token in doc:\n",
    "#     print(token.text)\n",
    "#     ancestors = [t.text for t in token.ancestors]\n",
    "#     print(ancestors)\n",
    "    \n",
    "# for token in doc:\n",
    "#     print(token.text)\n",
    "#     children = [t.text for t in token.children]\n",
    "#     print(children)\n",
    "    \n",
    "# for token in doc:\n",
    "#     print(token.text)\n",
    "#     subtree = [t.text for t in token.subtree]\n",
    "#     print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting sentences into clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He \t 0 \t PRON \t nsubj \t ['eats'] \t []\n",
      "eats \t 1 \t VERB \t ROOT \t [] \t ['He', 'cheese', ',', 'but', 'eat']\n",
      "cheese \t 2 \t NOUN \t dobj \t ['eats'] \t []\n",
      ", \t 3 \t PUNCT \t punct \t ['eats'] \t []\n",
      "but \t 4 \t CCONJ \t cc \t ['eats'] \t []\n",
      "he \t 5 \t PRON \t nsubj \t ['eat', 'eats'] \t []\n",
      "wo \t 6 \t AUX \t aux \t ['eat', 'eats'] \t []\n",
      "n't \t 7 \t PART \t neg \t ['eat', 'eats'] \t []\n",
      "eat \t 8 \t VERB \t conj \t ['eats'] \t ['he', 'wo', \"n't\", 'cream', '.']\n",
      "ice \t 9 \t NOUN \t compound \t ['cream', 'eat', 'eats'] \t []\n",
      "cream \t 10 \t NOUN \t dobj \t ['eat', 'eats'] \t ['ice']\n",
      ". \t 11 \t PUNCT \t punct \t ['eat', 'eats'] \t []\n",
      "['He eats cheese,', \"he won't eat ice cream\"]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"He eats cheese, but he won't eat ice cream.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    ancestors = [t.text for t in token.ancestors]\n",
    "    children = [t.text for t in token.children]\n",
    "    print(token.text, \"\\t\", token.i, \"\\t\", token.pos_, \"\\t\", token.dep_, \"\\t\", ancestors, \"\\t\", children)\n",
    "\n",
    "def find_root_of_sentence(doc):\n",
    "    root_token = None\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'ROOT':\n",
    "            root_token = token\n",
    "            break\n",
    "    return root_token\n",
    "\n",
    "root_token = find_root_of_sentence(doc)\n",
    "\n",
    "def find_other_verbs(doc, root_token):\n",
    "    other_verbs = []\n",
    "    for token in doc:\n",
    "        ancestors = list(token.ancestors)\n",
    "        if (token.pos_ == 'VERB' and len(ancestors) == 1 and ancestors[0] == root_token):\n",
    "            other_verbs.append(token)\n",
    "    return other_verbs\n",
    "\n",
    "other_verbs = find_other_verbs(doc, root_token)\n",
    "\n",
    "def get_clause_token_span_for_verb(verb, doc, all_verbs):\n",
    "    fist_token_index = len(doc)\n",
    "    last_token_index = 0\n",
    "    this_verb_children = list(verb.children)\n",
    "    for child in this_verb_children:\n",
    "        if (child not in all_verbs):\n",
    "            if (child.i < fist_token_index):\n",
    "                fist_token_index = child.i\n",
    "            if (child.i > last_token_index):\n",
    "                last_token_index = child.i\n",
    "    \n",
    "    return (fist_token_index, last_token_index)\n",
    "\n",
    "token_spans = []\n",
    "all_verbs = [root_token] + other_verbs\n",
    "for other_verb in all_verbs:\n",
    "    (first_token_index, last_token_index) = get_clause_token_span_for_verb(other_verb, doc, all_verbs)\n",
    "    token_spans.append((first_token_index, last_token_index))\n",
    "    \n",
    "sentence_clauses = []\n",
    "for token_span in token_spans:\n",
    "    start = token_span[0]\n",
    "    end = token_span[1]\n",
    "    if (start < end):\n",
    "        clause = doc[start:end]\n",
    "        sentence_clauses.append(clause)\n",
    "\n",
    "sentence_clauses = sorted(sentence_clauses, key=lambda tup: tup[0])\n",
    "\n",
    "causes_text = [clause.text for clause in sentence_clauses]\n",
    "\n",
    "print(causes_text)\n",
    "           \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
